---
title: "Lab2_EDA"
author: "Javier Rodriguez"
date: "2022-12-05"
output: html_document
---

# read csv file
```{r}
lab2_df = read.csv('lab2_data.csv')
print(lab2_df)
```
# load libraries
```{r}
install.packages("GGally")
install.packages("janitor")
```

# load libraries
```{r}
library(tidyverse)
library(ggplot2)
library(GGally)
library(sandwich)
library(stargazer)
library(janitor)
library(dplyr)
library(car)
library(lmtest)
library(gridExtra)
library(moments)
```

```{r}
library(stringr)
lab2_df <- lab2_df %>% mutate(words = str_count(lab2_df$title, pattern = "\\w+"))
```

# count of video_id = "#NAME?"
```{r}
sum(lab2_df$video_id == "#NAME?")
```

# count of views = NA
```{r}
sum(is.na(lab2_df$views))
```

# count of words = NA
```{r}
sum(is.na(lab2_df$words))
```

# count of words = 0
```{r}
sum(lab2_df$words == 0)
```

```{r}
sum(is.na(lab2_df$likes))
```

# count of likes = 0
```{r}
sum(lab2_df$likes == 0)
```

# distribution of views
```{r}
lab2_dfHist<-lab2_df %>%
  ggplot(aes(x = views)) + 
  geom_histogram() + 
  labs(
    x = 'views',
    title = 'Distribution of views'
  )

lab2_dfHist
```

# distribution of ln(views)
```{r}
lab2_dfLn_Hist<-lab2_df %>%
  ggplot(aes(x = log(views))) + 
  geom_histogram() + 
  labs(
    x = 'ln(views)',
    title = 'Distribution of ln(views)'
  )

lab2_dfLn_Hist
```

# summarize data
```{r}
qqView <- ggplot(lab2_df, aes(sample = lab2_df$views)) + geom_qq(distribution = qnorm) +
  geom_qq_line(line.p = c(0.25,0.75), col = "blue") + labs(y="views", title="qq normal plot of views")
qqView
```

# qqline
```{r}
qqLog <- ggplot( lab2_df, aes(sample = log( lab2_df$views))) + geom_qq(distribution = qnorm) +
  geom_qq_line(line.p = c(0.25,0.75), col = "blue") + labs(y="ln(views)", title="qq normal plot of ln(views)")
qqLog
```

# output the plots
```{r}
options(repr.plot.width=7.5, repr.plot.height=2)
suppressWarnings(grid.arrange(lab2_dfHist, lab2_dfLn_Hist, qqView, qqLog, ncol = 2, nrow = 2,
                              padding = unit(0.1, "line"), widths = c(1,1)))
```
# mean, median, fivenum
```{r}
mean(lab2_df$views,na.rm=T)
median(lab2_df$views,na.rm=T)
fivenum(lab2_df$views)
```
# if your median is less than your mean, than your data is positively skewed (median just looks at observations), if the same, then symmetrical distribution, if your median is more than your mean, than your data is negatively skewed

# output the plots
```{r}
mean(log(lab2_df$views),na.rm=T)
median(log(lab2_df$views),na.rm=T)
fivenum(log(lab2_df$views))
```
# if the same, then symmetrical distribution

#QQNorm plots of views
```{r}
par(mfrow=c(1,2))
qqnorm(lab2_df$views,main="QQNorm_views")
qqnorm(log(lab2_df$views),main="QQNorm_LN(views)")
par(mfrow=c(1,1))
```

#print some summary diagnostics francier
```{r}
print(sprintf("The mean number of views is %.0f",mean(lab2_df$views, na.rm = TRUE)))
print(sprintf("The median number of views is %.0f",median(lab2_df$views, na.rm = TRUE)))
print(sprintf("The minimum number of views is %.0f",min(lab2_df$views, na.rm = TRUE)))
print(sprintf("The 25th percentile video has %.0f views",quantile(lab2_df$views, na.rm = T, probs = c(0.25))))
print(sprintf("The 75th percentile video has %.0f views",quantile(lab2_df$views, na.rm = T, probs = c(0.75))))
print(sprintf("The maximum number of views is %.0f",max(lab2_df$views, na.rm = TRUE)))

print("")
print(sprintf("The mean number of ln(views) is %.2f",mean(log(lab2_df$views), na.rm = TRUE)))
print(sprintf("The median number of ln(views) is %.2f",median(log(lab2_df$views), na.rm = TRUE)))
```

# What are likes? Likes are the number likes a video recieves. Likes is rating assigned by users. Users like or dislike videos. Videos that have never been liked by anyone receive a zero.

# QQNorm plots of views
```{r}
par(mfrow=c(1,2))
hist(lab2_df$likes)
qqnorm(lab2_df$likes)
par(mfrow=c(1,1))
```

# mean likes
```{r}
mean_likes<-mean(lab2_df$likes,na.rm = TRUE)
mean_likes
```

# print summary diagnostics
```{r}
print(sprintf("The number of videos with a zero likes is %.0f", sum(lab2_df$likes==0, na.rm = TRUE)))
print(sprintf("The 99th percentile video (in terms of views) has %.0f views",quantile(lab2_df[lab2_df$views,"views"], na.rm = T, probs = c(0.99))))
```

#print some summary diagnostics francier
```{r}
print(sprintf("The mean rating (videos with likes only) is %.2f",mean(lab2_df[lab2_df$likes !=0,"views"], na.rm = TRUE)))
print(sprintf("The median rating (videos with likes only) is %.2f",median(lab2_df[lab2_df$likes !=0,"views"], na.rm = TRUE)))
print(sprintf("The minimum rating (videos with views only) is %.2f",min(lab2_df[lab2_df$likes !=0,"views"], na.rm = TRUE)))
print(sprintf("The 25th percentile video (video with likes only) is %.2f views",quantile(lab2_df[lab2_df$likes!=0,"views"], na.rm = T, probs = c(0.25))))
print(sprintf("The 75th percentile video (video with likes only) is %.2f views",quantile(lab2_df[lab2_df$likes!=0,"views"], na.rm = T, probs = c(0.75))))
print(sprintf("The maximum rating (videos with likes only) is %.2f",max(lab2_df[lab2_df$likes !=0,"views"], na.rm = TRUE)))

print("")
```
# Words. Words is the number of words in the title. Issues with words data. A toal of 584 videos are missiong views. Videos vary in words. 

#histograms of words
```{r}
wHist<-ggplot(data=lab2_df, aes(x=words))+
  geom_histogram(breaks=seq(-.125,20.125,by=.25), fill="black", alpha = .5,
                 aes(y=100*..count../sum(..count..))) + 
  scale_y_continuous(breaks = seq(0,10, by=5), limits=c(0,10)) +
  labs(title="Historgram of words",x="words",
       y="Percent of videos")
wHist
```

#make histogram of natural log of length
```{r}
lnwHist<-ggplot(data=lab2_df, aes(x=log(words)))+
  geom_histogram(breaks=seq(-.05,9.05,by=.1), fill="black", alpha = .5,
                 aes(y=100*..count../sum(..count..))) + 
  scale_y_continuous(breaks = seq(0,10, by=5), limits=c(0,10)) +
  labs(title="Historgram of ln(words)",x="ln(words)",
       y="Percent of videos")

lnwHist
```

#make a normal quantile-quantile plot for words
```{r}
qqLen <- ggplot(lab2_df, aes(sample = lab2_df$words)) + geom_qq(distribution = qnorm) + geom_qq_line(line.p = c(0.25,0.75), col = "blue") + labs(y="words", title = "qq normal plot of words")
qqLen
```

#output of plots
```{r}
options(repr.plot.width=7.5, repr.plot.height=2)
suppressWarnings(grid.arrange(wHist,qqLen,ncol=2,nrow=1,
                              padding = unit(0.1, "line"), widths = c(1,1)))
```
#
```{r}
par(mfrow=c(1,2))
hist(lab2_df$words)
qqnorm(lab2_df$words)
par(mfrow=c(1,1))
```
#print some summary diagnostics
```{r}
print(sprintf("%.0f videos are longer than 5 words.", sum(lab2_df$words > 5, na.rm = TRUE)))
print(sprintf("%.1f percent of videos are longer than 5 words.",sum(lab2_df$words > 5, na.rm = TRUE)))

print("")
print(sprintf("The median number of views for videos over 5 words is %.0f,  words.",100.0*sum(lab2_df$words > 5, na.rm = TRUE)))
```
#
```{r}
print(sprintf("The median number of views for 1 word title videos is %.0f",median(lab2_df[lab2_df$words == 1, "views"],na.rm=TRUE)))
print(sprintf("The median number of views for 2 word title videos is %.0f",median(lab2_df[lab2_df$words == 2, "views"],na.rm=TRUE)))
print(sprintf("The median number of views for 3 word title videos is %.0f",median(lab2_df[lab2_df$words == 3, "views"],na.rm=TRUE)))
print(sprintf("The median number of views for 4 word title videos is %.0f",median(lab2_df[lab2_df$words == 4, "views"],na.rm=TRUE)))
print(sprintf("The median number of views for 5 word title videos is %.0f",median(lab2_df[lab2_df$words == 5, "views"],na.rm=TRUE)))
print(sprintf("The median number of views for 6 word title videos is %.0f",median(lab2_df[lab2_df$words == 6, "views"],na.rm=TRUE)))
print(sprintf("The median number of views for 7 word title videos is %.0f",median(lab2_df[lab2_df$words == 7, "views"],na.rm=TRUE)))
print(sprintf("The median number of views for videos with 8 word title is %.0f",median(lab2_df[lab2_df$words == 8, "views"],na.rm=TRUE)))
print(sprintf("The median number of views for videos with 9 word title is %.0f",median(lab2_df[lab2_df$words == 9, "views"],na.rm=TRUE)))
print(sprintf("The median number of views for videos with 10 word title is %.0f",median(lab2_df[lab2_df$words == 10, "views"],na.rm=TRUE)))
print(sprintf("The median number of views for videos with 11+ word title is %.0f",median(lab2_df[lab2_df$words >= 11, "views"],na.rm=TRUE)))
```

# 
```{r}
mean(lab2_df$words,na.rm=T)
median(lab2_df$words,na.rm=T)
fivenum(lab2_df$words)

print(sprintf("The mean words is %.2f words", mean(lab2_df$words, na.rm=TRUE)))
print(sprintf("The median words is %.2f words", median(lab2_df$words,na.rm=TRUE)))
```
# remove NAS
```{r}
lab2_df <- lab2_df[!is.na(lab2_df$views),]
```

# count of views = NA
```{r}
sum(is.na(lab2_df$views))
```

# count of words = NA
```{r}
sum(is.na(lab2_df$words))
```

# count of words = 0
```{r}
sum(lab2_df$words == 0)
```

# count of words = 0
```{r}
sum(lab2_df$likes == 0)
```

# assign ratings to the unrated videos
```{r}
dim(lab2_df)
```

#
```{r}
cor(cbind(log(lab2_df$views), lab2_df$views,lab2_df$likes,lab2_df$words,deparse.level = 2))
df <- as.data.frame(cbind(log(lab2_df$views), lab2_df$views, lab2_df$likes, lab2_df$words))
```
#
```{r}
pairs(~log(lab2_df$views) + lab2_df$views + lab2_df$likes + lab2_df$words,
      lower.panel = panel.smooth)
```

#scatterplot
```{r}
scatterplotMatrix(~log(lab2_df$views) + lab2_df$views + lab2_df$likes + lab2_df$words,
                  data = NULL, plot.points = F)
```

#
```{r}
df %>%
  ggpairs(columns = c("Log_Views","Views","Likes","Words"),
          upper = list(continuous=wrap('cor',size=3)))
```
#add a vector for words
```{r}
lab2_df$lenMin<-lab2_df$words
```

#create model
```{r}
model1 <- lm(log(views) ~ likes + lenMin, data=lab2_df)

model1
```

#create model diagnostic plots

#Assumptions
#1. Linearity C.1
#2. IID C.2
#3. Zero Conditional Mean
#4. Homoscedasticity C.3
#5. No Perfect Multicolinear C.4
#6. Normality of Errors C.5

#Which-which?, which is a subset of the numbers 1:6, by default 1:3, 5, referring to 
#1. Residuals vs Fitted", aka 'Tukey-Anscombe' plot
#2. "Normal Q-Q" plot, an enhanced qqnorm(resid(.))
#3. "Scale-Location"
#4. "Cook's distance"
#5. "Residuals vs Leverage"
#6. "Cook's dist vs Lev./(1-Lev.)"

#Outliers
```{r}
plot(model1,which=4)
thresCook <- 4/(dim(df2)[1] - length(model1$coefficients))
thresCook
abline(h=thresCook, col='red')
```

#print summary statistics for model
```{r}
print("Model Summary Data - Non-heteroskedasticity-robost Standards Errors")
summary(model1)
```

#
```{r}
print("Coefficients with Heteroskedasticity-robust Standard Errors")
coeftest(model1, vcov = vcovHC)
```

# create a variable to hold my robust standard errors
```{r}
se.model1 <- sqrt(diag(vcovHC(model1)))
```

# display the regression table with those robust errors
```{r}
suppressWarnings(stargazer(model1,type = "text",omit.stat = "f",
                           se = list(se.model1),
                           star.cutoffs = c(0.05, 0.01, 0.001)))
```

#Coef Interpretation
```{r}
exp(2.875e-05)
exp(-4.861e-02)
```

# adding package zip
```{r}
install.packages("zip")
```

#additional libraries
```{r}
install.packages("olsrr")
install.packages("jtools")
install.packages("moments")
install.packages("lmtest")
```

# load libraries
```{r}
library(olsrr)
library(jtools)
library(moments)
library(lmtest)
```

# parameters for the regression output
```{r}
my_confidence <- 0.95
my_digits <- 3
```

# data for linearity check model1 <- lm(log(views) ~ rate + lenMin, data=df2)
```{r}
attach(lab2_df)
daten.plot <- data.frame(log(views), likes,lenMin)
detach(lab2_df)
```

# 1 Regression output

# 1.1 Unstandardized results
```{r}
summ(model1, confint=TRUE, ci.width = my_confidence,
digits = my_digits)
```

# 1.2 Standardized results
```{r}
summ(model1, scale=TRUE, transform.response = TRUE, digits=my_digits)
```

# Assumption: No perfect collinearity 
# This assumption is challenged if the VIF values are above 10.0.

# Test of collinearity
```{r}
ols_vif_tol(model1)
```

# Given that VIF values < 10.0, it is safe to assume no perfect collinearity.

#3.1 Pairwise scatterplots (Only the scatterplots that include the 
#criterion variable are relevant)
```{r}
pairs(daten.plot, pch = 19, lower.panel = NULL)
```

# 3.2 Rainbow test (with lmtest-Package) for linearity (significant => nonlinearity)
```{r}
raintest(model1)
```

# The p-value is greater than the significance value. Since we fail to reject 
# the null hypothesis, regression is linear. Thus, it is safe to assume 
# the linear conditional expectation.

# 4.1.1 Graphical test (should be a chaotic point cloud; problematic especially
#a funnel shape or a recognizably curved structure)
```{r}
ols_plot_resid_fit(model1)
```

# 4.1.2 Breusch Pagan Test - significance test for heteroskedasticity
# (significant => heteroskedasticity)
```{r}
ols_test_breusch_pagan(model1)
```

# The Breush-Pagan test creates a statistic that is chi-squared distributed and 
#for the data the statistic = 2.252744. The p-value is the result of the 
#chi-squared test and (normally) the null hypothesis is rejected for 
#p-value < 0.05. In this case, the null hypothesis is of homoskedasticity 
#and it would be rejected.

# 5 Normally Distributed Errors

# 5.1 Histogram of residuals (The histogramm should show a normal distribution,
# especially at the tails of the distribution)
```{r}
ols_plot_resid_hist(model1)
```

# 5.2 QQ plot (The data points should be near the diagonal)
```{r}
ols_plot_resid_qq(model1)
```

# 5.3.1 Skewness (For normality skewness near 0)
```{r}
skewness(model1$residuals)
```

# 5.3.2 Skewness (For normality skewness near 0) significance test for skewness
#(significant => residuals not normally distributed)
```{r}
agostino.test(model1$residuals)
```

# 5.4.1 Kurtosis (For normality kurtosis near 3)
```{r}
kurtosis(model1$residuals)
```

# 5.4.2 Kurtosis (For normality kurtosis near 3) significance test for kurtosis
#(significant => residuals not normally distributed)
```{r}
anscombe.test(model1$residuals)
```